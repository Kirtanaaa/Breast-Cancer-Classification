{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjT8xs8B/vGjJkCoaOvodY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kirtanaaa/Breast-Cancer-Classification/blob/main/bcan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BREAST CANCER CLASSIFICATION"
      ],
      "metadata": {
        "id": "hzanAcMgHP3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91vVy2GYHFY5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = pd.read_csv('bcan.csv')\n",
        "x = ds.iloc[:,:-1].values\n",
        "y = ds.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "6tFgoaLEJvDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)"
      ],
      "metadata": {
        "id": "TpNDDcM0J87u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)"
      ],
      "metadata": {
        "id": "P-9xP1knKQAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Classification Model"
      ],
      "metadata": {
        "id": "mYBgPZ_2KkLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a machine learning algorithm used for binary classification tasks, where the goal is to predict one of two possible classes (e.g., Yes/No, Spam/Not Spam, etc.).\n",
        "\n",
        "- Input: It takes one or more features (numerical or categorical) as input. For binary classification, we have two classes, labeled as 0 and 1.\n",
        "\n",
        "- Output: The algorithm outputs a probability score between 0 and 1, indicating the likelihood of an input belonging to class 1. If the probability is close to 0, it belongs to class 0, and if it's close to 1, it belongs to class 1.\n",
        "\n",
        "- Modeling: Logistic regression uses the sigmoid function to model the relationship between the input features and the predicted probability. The sigmoid function ensures that the output stays within the range (0, 1).\n",
        "\n",
        "- Training: During training, the algorithm adjusts the model's parameters (weights) to minimize the difference between the predicted probabilities and the actual class labels in the training data.\n",
        "\n",
        "- Decision Boundary: The decision boundary is a threshold value (usually 0.5) that separates the two classes. If the predicted probability is greater than the threshold, the instance is classified as class 1; otherwise, it's classified as class 0.\n",
        "\n",
        "\n",
        "Applications:\n",
        "\n",
        "Logistic regression has various applications in real-world scenarios, including:\n",
        "\n",
        "- Email Spam Detection: To classify incoming emails as spam or not spam based on their content and features.\n",
        "\n",
        "- Medical Diagnosis: For predicting whether a patient has a particular disease based on medical test results and patient characteristics.\n",
        "\n",
        "- Credit Risk Assessment: To determine the likelihood of a customer defaulting on a loan based on their credit history and financial attributes.\n",
        "\n",
        "- Customer Churn Prediction: To predict whether a customer will cancel their subscription or leave a service based on historical usage data and behavior.\n",
        "\n",
        "- Online Ad Click Prediction: To predict the probability of a user clicking on an online advertisement based on their browsing behavior and demographics.\n",
        "\n",
        "- Sentiment Analysis: To determine whether a piece of text (e.g., a review or tweet) expresses a positive or negative sentiment.\n"
      ],
      "metadata": {
        "id": "p6MB_OOFC5_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-class Logistic Regression:\n",
        "\n",
        "In the case of multi-class classification (more than two classes), logistic regression can be adapted to handle multiple classes by using the following approaches:\n",
        "\n",
        "1. One-vs-Rest (OvR) or One-vs-All (OvA):\n",
        "\n",
        "In this strategy, you train multiple binary logistic regression classifiers, where each classifier is responsible for distinguishing one class from all the others. For example, if you have three classes (A, B, C), you would train three binary classifiers: A vs. (B, C), B vs. (A, C), and C vs. (A, B). During prediction, the class with the highest probability from the individual classifiers is assigned as the final class label.\n",
        "\n",
        "2. Softmax Regression (Multinomial Logistic Regression):\n",
        "\n",
        "Softmax regression generalizes logistic regression to handle multiple classes directly. Instead of predicting a probability for just one class, it predicts a probability distribution across all classes. The probabilities are normalized using the softmax function, which ensures that they sum to 1. The class with the highest probability is considered the predicted class.\n",
        "\n",
        "Applications of Multi-class Logistic Regression:\n",
        "\n",
        "- Handwritten Digit Recognition: Classifying images of handwritten digits (0 to 9) into their corresponding classes.\n",
        "\n",
        "- Species Classification: Identifying different species of plants or animals based on various features.\n",
        "\n",
        "- Sentiment Analysis (with multiple classes): Categorizing text into several sentiment classes like positive, negative, and neutral.\n",
        "Image Classification: Identifying objects or scenes in images from a predefined set of classes."
      ],
      "metadata": {
        "id": "rCyrtzwWDsGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier1 = LogisticRegression()\n",
        "classifier1.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "0NPxwyqsKoSe",
        "outputId": "41ef245a-0f32-4608-a6c2-8cf2cadb7576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-22 {color: black;background-color: white;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = classifier1.predict(x_test)\n",
        "print(np.concatenate((y_pred1.reshape(len(y_pred1),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "e5wvdw7SLBle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "cm1 = confusion_matrix(y_test,y_pred1)\n",
        "print(cm1)\n",
        "accuracy_score(y_test,y_pred1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIMjs4HPLc5p",
        "outputId": "30f543d5-71ce-4ab3-8fd7-bffe6b665f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[105   3]\n",
            " [  3  60]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9649122807017544"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Classification Model"
      ],
      "metadata": {
        "id": "_0y8VFIjL0VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) Classification:\n",
        "\n",
        "K-Nearest Neighbors is a simple yet powerful classification algorithm used in supervised machine learning. It's a type of instance-based learning, where the algorithm doesn't build an explicit model but instead memorizes the training dataset. KNN makes predictions based on the majority class of its k-nearest neighbors in the feature space.\n",
        "\n",
        "How KNN Works:\n",
        "\n",
        "1. Data Collection and Preprocessing: Just like with other machine learning algorithms, you start by collecting and preprocessing your data. Ensure that your features are appropriately scaled and relevant for the classification task.\n",
        "\n",
        "2. Choosing k: The \"k\" in KNN represents the number of neighbors that will be considered when making a prediction. Choosing an appropriate k value is crucial. A small k value can be sensitive to noise, while a large k value can make the model less flexible. The choice of k should be based on cross-validation or other techniques.\n",
        "\n",
        "3. Calculating Distances: KNN works by calculating the distances between data points in the feature space. Common distance metrics include Euclidean distance, Manhattan distance, and cosine similarity. The distance metric you choose depends on the nature of your data.\n",
        "\n",
        "4. Finding Nearest Neighbors: Once the distances are calculated, the algorithm identifies the k-nearest neighbors to the data point you're trying to classify.\n",
        "\n",
        "5. Voting or Weighting: For classification tasks, the algorithm looks at the class labels of the k-nearest neighbors. It either counts the votes from each class and assigns the class with the most votes to the data point, or it can apply weighted voting, where closer neighbors have a greater influence on the prediction.\n",
        "\n",
        "6. Making Predictions: After voting or weighting, the class label with the highest count (or weighted score) among the k-nearest neighbors is assigned to the data point being classified.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's say you're working on a project to classify whether an email is spam or not spam (ham). You've collected a dataset with features like the frequency of certain words and the length of the email. To make a prediction for a new email, the KNN algorithm would:\n",
        "\n",
        "1. Calculate distances between the new email's feature values and all the labeled emails in the dataset.\n",
        "\n",
        "2. Identify the k-nearest labeled emails based on the shortest distances.\n",
        "\n",
        "3. Count the number of spam and ham emails among the k-nearest neighbors.\n",
        "\n",
        "4. Assign the class label (spam or ham) based on the majority count.\n",
        "\n",
        "Unique Aspect:\n",
        "\n",
        "One unique feature of KNN is its simplicity and versatility. It doesn't make strong assumptions about the underlying data distribution, making it suitable for a wide range of problems. Moreover, it can be used for both classification and regression tasks. KNN also showcases the importance of distance metrics and the significance of choosing the right k value. The trade-off between bias and variance (lower k tends to overfit, higher k tends to smooth out data) is a critical aspect to consider.\n",
        "\n",
        "In summary, K-Nearest Neighbors is a powerful and intuitive classification algorithm that makes predictions based on the majority class of its k-nearest neighbors. Its simplicity and flexibility have made it a popular choice in various domains, from image recognition to recommendation systems. Just remember to preprocess your data, choose an appropriate k value, and select a suitable distance metric for your specific problem."
      ],
      "metadata": {
        "id": "vll4ihcHkBg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier2 = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\n",
        "classifier2.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "qUAFVmL1MEei",
        "outputId": "1155be5a-8c40-42eb-8fc8-39db5718c1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-23 {color: black;background-color: white;}#sk-container-id-23 pre{padding: 0;}#sk-container-id-23 div.sk-toggleable {background-color: white;}#sk-container-id-23 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-23 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-23 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-23 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-23 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-23 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-23 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-23 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-23 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-23 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-23 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-23 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-23 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-23 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-23 div.sk-item {position: relative;z-index: 1;}#sk-container-id-23 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-23 div.sk-item::before, #sk-container-id-23 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-23 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-23 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-23 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-23 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-23 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-23 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-23 div.sk-label-container {text-align: center;}#sk-container-id-23 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-23 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-23\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = classifier2.predict(x_test)\n",
        "print(np.concatenate((y_pred2.reshape(len(y_pred2),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "oGws9Xq-MbV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm2 = confusion_matrix(y_test,y_pred2)\n",
        "print(cm2)\n",
        "accuracy_score(y_test,y_pred2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWL571GUMgA9",
        "outputId": "21959cca-5f0a-443a-f4b2-334df161d635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[107   1]\n",
            " [  6  57]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9590643274853801"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support Vector Machine Classification Model"
      ],
      "metadata": {
        "id": "M50pycN2E5Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines are powerful and versatile machine learning algorithms commonly used for both classification and regression tasks. They belong to the family of supervised learning algorithms, meaning they require labeled data for training. SVMs are particularly well-suited for cases where the data isn't linearly separable in its original feature space. They work by finding a hyperplane that best separates different classes of data points while maximizing the margin between them.\n",
        "\n",
        "Important Terms:\n",
        "\n",
        "1. Hyperplane:\n",
        "\n",
        "A hyperplane is a decision boundary that separates the data into different classes. In two-dimensional space, it's a line, in three-dimensional space, it's a plane, and so on.\n",
        "\n",
        "2. Margin:\n",
        "\n",
        "The margin is the distance between the hyperplane and the nearest data points from each class. SVM aims to maximize this margin.\n",
        "\n",
        "3. Support Vectors:\n",
        "\n",
        "These are the data points that lie closest to the decision boundary (hyperplane). They play a critical role in defining the hyperplane.\n",
        "\n",
        "4. Kernel Trick:\n",
        "\n",
        "SVM can handle non-linear data by transforming the original feature space into a higher-dimensional space using a kernel function. This allows SVM to find a linear hyperplane in the transformed space.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- SVMs are effective in high-dimensional spaces, making them suitable for tasks like text categorization and image classification.\n",
        "\n",
        "- They can handle both linear and non-linear data through the use of different kernel functions.\n",
        "\n",
        "- SVMs are less prone to overfitting when compared to some other complex models.\n",
        "\n",
        "- They work well when there's a clear margin of separation between classes.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- SVMs can be computationally expensive, especially with large datasets.\n",
        "\n",
        "- Choosing the right kernel function and tuning hyperparameters can be challenging.\n",
        "\n",
        "- Interpreting the results can be less intuitive compared to simpler models like logistic regression.\n",
        "- SVMs might struggle with noisy data or overlapping classes.\n",
        "\n",
        "Applications:\n",
        "\n",
        "1. Image Recognition: They are used for object recognition, face detection, and image classification.\n",
        "\n",
        "2. Text Classification: SVMs are effective in sentiment analysis, topic classification, and spam detection.\n",
        "\n",
        "3. Bioinformatics: They're used in protein classification and gene expression analysis.\n",
        "\n",
        "4. Finance: SVMs can predict stock prices and detect fraudulent activities.\n",
        "Medical Diagnosis: They assist in disease classification and prognosis.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you have a dataset of flower features (like petal length, petal width, etc.) and you want to classify them into different species (e.g., iris-setosa, iris-versicolor, iris-virginica). By training an SVM model on this data, the algorithm will find the best hyperplane that separates the flowers into their respective classes."
      ],
      "metadata": {
        "id": "Z4WvmR6amqDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier3 = SVC(kernel='linear')\n",
        "classifier3.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "90QxkII2GORO",
        "outputId": "db749c0e-1381-4bff-d33c-1842f0d881df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ],
            "text/html": [
              "<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred3 = classifier3.predict(x_test)\n",
        "print(np.concatenate((y_pred3.reshape(len(y_pred3),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "esIZCTJFH16j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm3 = confusion_matrix(y_test,y_pred3)\n",
        "print(cm3)\n",
        "accuracy_score(y_test,y_pred3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd84AOvtIRyZ",
        "outputId": "645275ba-b977-4426-9337-0a13141597d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[100   8]\n",
            " [  3  60]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.935672514619883"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernal Support Vector Machine Classification Model"
      ],
      "metadata": {
        "id": "izJeFEUpJHiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Kernel SVM is an enhancement of the traditional SVM that introduces the concept of kernel functions. These functions allow the SVM to transform the original feature space into a higher-dimensional space where the data might become linearly separable. This transformation enables the SVM to handle complex, non-linear relationships between features.\n",
        "\n",
        "Important Terms:\n",
        "\n",
        "1. Kernel Function:\n",
        "\n",
        "A kernel function computes the dot product between two data points in the transformed space. Common kernel functions include:\n",
        "\n",
        "2. Linear Kernel: It's equivalent to the standard SVM without any transformation.\n",
        "\n",
        "3. Polynomial Kernel:\n",
        "\n",
        "Raises the dot product to a power, creating a polynomial effect.\n",
        "\n",
        "4. Gaussian (RBF) Kernel:\n",
        "\n",
        "Utilizes a Gaussian distribution to transform data into a higher-dimensional space.\n",
        "\n",
        "5. Sigmoid Kernel:\n",
        "\n",
        "Produces an S-shaped transformation, useful for neural network activation functions.\n",
        "\n",
        "6. Regularization:\n",
        "\n",
        "Like in standard SVM, regularization parameter (C) is used to control the trade-off between maximizing the margin and minimizing the classification error. A higher C emphasizes correct classification, while a lower C emphasizes a larger margin.\n",
        "\n",
        "7. Margin and Support Vectors:\n",
        "\n",
        "These terms retain their meanings from standard SVM. Support vectors in the transformed space still define the optimal hyperplane.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- Non-linearity Handling: Kernel SVMs can handle complex non-linear data patterns, which standard SVMs cannot do without transformation.\n",
        "\n",
        "- Flexible Kernel Choices: Different kernel functions can be used based on the problem's nature and requirements.\n",
        "\n",
        "- Powerful in High Dimensions: Kernel SVMs can work well in high-dimensional spaces, such as text data, where relationships might be intricate.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Hyperparameter Tuning: Selecting the right kernel and its parameters can be challenging and require experimentation.\n",
        "\n",
        "- Computational Intensity: Kernel transformations can be computationally expensive, especially with large datasets.\n",
        "\n",
        "- Overfitting: If not properly tuned, kernel SVMs can overfit to noise or small datasets.\n",
        "\n",
        "Applications:\n",
        "\n",
        "Kernel SVMs are particularly useful in cases where data has non-linear relationships:\n",
        "\n",
        "1. Image Segmentation: Identifying objects in images where boundaries are not linear.\n",
        "\n",
        "2. Text Sentiment Analysis: Analyzing sentiment in text data where the correlation between words might not be linear.\n",
        "\n",
        "3. Biomedical Data: In areas like genomics, where the relationship between genes might not be linear.\n",
        "\n",
        "Example:\n",
        "\n",
        "Consider a scenario where you're working with a dataset of medical test results, and you want to predict whether a patient has a certain disease or not. The relationship between different test results might not be linear, but by using a suitable kernel function (e.g., Gaussian kernel), you can transform the data to a space where a linear boundary is effective for classification."
      ],
      "metadata": {
        "id": "8wMzoJqhqbOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier4 = SVC(kernel='rbf')\n",
        "classifier4.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "oreDk8rdJQy9",
        "outputId": "c8b0f1a5-8b7f-49e1-feb0-003a0c0fa5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ],
            "text/html": [
              "<style>#sk-container-id-25 {color: black;background-color: white;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred4 = classifier4.predict(x_test)\n",
        "print(np.concatenate((y_pred4.reshape(len(y_pred4),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "Ap3LqWMFJefh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm4 = confusion_matrix(y_test,y_pred4)\n",
        "print(cm4)\n",
        "accuracy_score(y_test,y_pred4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT0wD7YpJnVr",
        "outputId": "5a98116a-1aea-4ac0-b8ad-ed42a036c6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[106   2]\n",
            " [  3  60]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9707602339181286"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Classification Model"
      ],
      "metadata": {
        "id": "y0XiSfQiJvdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem, with the \"naive\" assumption of independence between features. Despite this assumption, Naive Bayes often performs surprisingly well in practice and is widely used in various applications like text classification, spam filtering, and sentiment analysis.\n",
        "\n",
        "Important Terms:\n",
        "\n",
        "1. Bayes' Theorem:\n",
        "\n",
        "A fundamental concept in probability theory that describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
        "\n",
        "2. Prior Probability:\n",
        "\n",
        "The initial probability of a certain class being true, before considering any evidence.\n",
        "\n",
        "3. Likelihood:\n",
        "\n",
        "The likelihood measures how likely the observed data is given the class.\n",
        "\n",
        "4. Posterior Probability:\n",
        "\n",
        "The probability of a certain class being true after considering the observed data.\n",
        "\n",
        "5. Conditional Independence:\n",
        "\n",
        "The \"naive\" assumption that the features are conditionally independent of each other given the class label.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- Simple and Fast: Naive Bayes is relatively simple to understand and implement. It's particularly fast for training and prediction.\n",
        "\n",
        "- Handles High Dimensions: Naive Bayes can handle high-dimensional data effectively, making it useful for text classification tasks.\n",
        "\n",
        "- Works with Small Datasets: Even with limited training data, Naive Bayes can still produce reasonable results.\n",
        "\n",
        "- Interpretable Results: The probabilistic nature of Naive Bayes provides a clear indication of how certain a prediction is.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Strong Independence Assumption: The naive assumption of feature independence may not hold in all real-world scenarios.\n",
        "\n",
        "- Sensitive to Feature Choice: The choice of features can heavily influence the model's performance.\n",
        "\n",
        "- Limited Expressiveness: Due to its simplicity, Naive Bayes might not capture complex relationships in data as well as more advanced models.\n",
        "\n",
        "- Poor Handling of Rare Events: If a feature's value is not present in the training data for a particular class, Naive Bayes cannot predict that class.\n",
        "\n",
        "Applications:\n",
        "\n",
        "Naive Bayes finds applications in text classification and other areas:\n",
        "\n",
        "1. Text Classification: Naive Bayes is often used for spam filtering, sentiment analysis, and topic categorization in natural language processing.\n",
        "\n",
        "2. Medical Diagnosis: It can assist in predicting medical conditions based on patient symptoms and test results.\n",
        "\n",
        "3. Recommendation Systems: Naive Bayes can contribute to building simple recommendation systems.\n",
        "\n",
        "4. Document Classification: It's used to categorize documents into predefined classes, like news articles or research papers.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you're building a spam email filter. By training a Naive Bayes model on a dataset of labeled emails (spam or not spam) with their word frequencies, the model learns the conditional probabilities of words given the class labels. When presented with a new email, the model calculates the probability of it being spam or not spam based on the words it contains and their corresponding probabilities.\n",
        "\n",
        "Also,\n",
        "suppose you want to find probability for your marks and there are 2 features:\n",
        "1. how much you study\n",
        "2. teachers partiality\n",
        "\n",
        "Naive bayes assumes there is no relation between the 2 and gives you independant probablities for each features without considering if the other ones might have a say in it. this is useful in some cases."
      ],
      "metadata": {
        "id": "Nmjh470S2YZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier5 = GaussianNB()\n",
        "classifier5.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "xK9q8kKPJ-rs",
        "outputId": "80558622-398f-415e-d5f0-c7f6ba5a2668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-26 {color: black;background-color: white;}#sk-container-id-26 pre{padding: 0;}#sk-container-id-26 div.sk-toggleable {background-color: white;}#sk-container-id-26 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-26 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-26 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-26 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-26 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-26 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-26 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-26 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-26 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-26 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-26 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-26 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-26 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-26 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-26 div.sk-item {position: relative;z-index: 1;}#sk-container-id-26 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-26 div.sk-item::before, #sk-container-id-26 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-26 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-26 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-26 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-26 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-26 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-26 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-26 div.sk-label-container {text-align: center;}#sk-container-id-26 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-26 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-26\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" checked><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred5 = classifier5.predict(x_test)\n",
        "print(np.concatenate((y_pred5.reshape(len(y_pred5),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "on2jzPayKL6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm5 = confusion_matrix(y_test,y_pred4)\n",
        "print(cm5)\n",
        "accuracy_score(y_test,y_pred5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EgPfHh9KVPa",
        "outputId": "839b4d3b-51ee-412e-9fb0-184d15f9545f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[106   2]\n",
            " [  3  60]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9122807017543859"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier6 = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
        "classifier6.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "95eBAvECKhm0",
        "outputId": "c9798c61-02dc-4cd6-e92b-444d524514ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy', random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-29 {color: black;background-color: white;}#sk-container-id-29 pre{padding: 0;}#sk-container-id-29 div.sk-toggleable {background-color: white;}#sk-container-id-29 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-29 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-29 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-29 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-29 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-29 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-29 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-29 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-29 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-29 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-29 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-29 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-29 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-29 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-29 div.sk-item {position: relative;z-index: 1;}#sk-container-id-29 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-29 div.sk-item::before, #sk-container-id-29 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-29 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-29 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-29 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-29 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-29 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-29 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-29 div.sk-label-container {text-align: center;}#sk-container-id-29 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-29 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-29\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" checked><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred6 = classifier6.predict(x_test)\n",
        "print(np.concatenate((y_pred6.reshape(len(y_pred6),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "BIE3L-R1K33W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm6 = confusion_matrix(y_test,y_pred6)\n",
        "print(cm6)\n",
        "accuracy_score(y_test,y_pred6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWvOwZh8K-4b",
        "outputId": "e687b075-ec7b-48ca-ebcc-83c3addc6e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[105   3]\n",
            " [  3  60]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9649122807017544"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classification Model"
      ],
      "metadata": {
        "id": "etA40Zwr18Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning algorithm that combines multiple decision trees to create a more robust and accurate classification model. It's designed to reduce overfitting and improve the generalization performance of individual decision trees.\n",
        "\n",
        "Important Terms:\n",
        "\n",
        "1. Ensemble Learning:\n",
        "\n",
        "A technique that combines multiple individual models to create a stronger overall model. In the case of Random Forest, the individual models are decision trees.\n",
        "\n",
        "2. Decision Tree:\n",
        "\n",
        "A tree-like structure where each internal node represents a decision based on a certain feature, leading to branches representing the possible outcomes or classes.\n",
        "\n",
        "3. Bagging (Bootstrap Aggregating):\n",
        "\n",
        "The process of creating multiple subsets (bags) of the training data by random sampling with replacement. Each subset is then used to train a separate decision tree.\n",
        "\n",
        "4. Feature Randomization:\n",
        "\n",
        "For each decision tree, a random subset of features is chosen to determine the best split at each node. This reduces the correlation between trees and improves diversity.\n",
        "\n",
        "5. Voting:\n",
        "\n",
        "In classification, each decision tree \"votes\" for a class, and the class with the most votes becomes the predicted class of the Random Forest.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- Reduces Overfitting: Random Forest reduces overfitting by averaging the predictions of multiple trees, which reduces the risk of any individual tree capturing noise in the data.\n",
        "\n",
        "- Handles High Dimensions: It can handle a large number of features and high-dimensional data effectively.\n",
        "\n",
        "- Robust to Outliers: Random Forest is less sensitive to outliers compared to individual decision trees.\n",
        "\n",
        "- Feature Importance: It provides a measure of feature importance, helping to identify which features contribute the most to the classification.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Complexity: Random Forest can be more complex to understand and interpret compared to single decision trees.\n",
        "\n",
        "- Computationally Intensive: Training multiple decision trees can be computationally expensive, especially with large datasets.\n",
        "\n",
        "- Possible Bias: If one class dominates the data, Random Forest may be biased towards that class.\n",
        "\n",
        "Applications:\n",
        "\n",
        "- Image Classification: It's used for object recognition, facial recognition, and scene classification.\n",
        "\n",
        "- Healthcare: Predicting medical conditions, identifying diseases, and prognosis.\n",
        "\n",
        "- Finance: Credit risk assessment, fraud detection, and stock market prediction.\n",
        "\n",
        "- Ecology: Species classification based on environmental factors.\n",
        "\n",
        "Example:\n",
        "\n",
        "Consider a scenario where you're building a model to classify whether a customer will purchase a product based on their demographic and browsing history. By training a Random Forest model on historical customer data, the ensemble of decision trees can provide a more accurate prediction by considering various features and their interactions."
      ],
      "metadata": {
        "id": "FpPIBv_k5EOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier7 = RandomForestClassifier(n_estimators=10,criterion='entropy')\n",
        "classifier7.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "6imSUkAd2b79",
        "outputId": "021041ff-65f6-41be-9fb1-ac888d776cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=10)"
            ],
            "text/html": [
              "<style>#sk-container-id-28 {color: black;background-color: white;}#sk-container-id-28 pre{padding: 0;}#sk-container-id-28 div.sk-toggleable {background-color: white;}#sk-container-id-28 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-28 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-28 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-28 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-28 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-28 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-28 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-28 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-28 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-28 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-28 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-28 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-28 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-28 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-28 div.sk-item {position: relative;z-index: 1;}#sk-container-id-28 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-28 div.sk-item::before, #sk-container-id-28 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-28 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-28 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-28 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-28 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-28 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-28 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-28 div.sk-label-container {text-align: center;}#sk-container-id-28 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-28 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-28\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" checked><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=10)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred7 = classifier7.predict(x_test)\n",
        "print(np.concatenate((y_pred7.reshape(len(y_pred7),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "4i8qKqD63UUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm7 = confusion_matrix(y_test,y_pred7)\n",
        "print(cm7)\n",
        "accuracy_score(y_test,y_pred7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkSMCwE53b34",
        "outputId": "ad30fcaf-66a2-4745-a4a5-ad19093ef0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[104   4]\n",
            " [  5  58]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ]
}